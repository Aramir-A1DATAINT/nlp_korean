{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정체(Cleaning) and 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정제(cleaning) : 노이즈 데이터 제거 \n",
    "- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 규칙에 기반한 표기가 다른 단어들 통합 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  같은 의미를 갖고 있음에도 다른 단어들을 하나로 정규화시키는 작업(normalization)\n",
    "- USA, US 같은 의미를 가지므로 하나로 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대, 소문자 통합 (영어)\n",
    "\n",
    "### 불필요한 단어의 제거(Removing Unnecessary Words)\n",
    "- 아무 의미가 없는 특수 문자, 분석에서 맞지 않은 불필요한 단어 (noise data) 제거\n",
    "- 1) Removing Rare words \n",
    "- 2) Removing words with a very short length\n",
    "    - 영어 단어의 평균 길이 6 ~ 7\n",
    "    - 한국어 단어의 평균 길이 2 ~3 \n",
    "    - 한국어 같은 경우 한 글자가 가진 의미가 크기가 다름 글자 하나가 함춤적인 의미를 가질수 있어서 \n",
    "    - 한국어 데이터인 경우 안 유용할수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어간 추출(Stemming) N 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표제어(Lemma) 표제어, 기본 사전형 단어\n",
    "- 표제어 추출을 표제어를 찾아가는 과정\n",
    "    - eg: am, are, is --> be \n",
    "\n",
    "> 표제어 추출 하는 방법\n",
    "- 형태학적 파싱 \n",
    "- 형태학(morphology)이란 형태소로 부터 단어를 만들어가능 학문\n",
    "- 형태소 `의미가 가진 가장 작은 단위`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 \n",
    "- 1) 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심\n",
    "- 2) 접사(affix) : 단어에 추가적인 의미를 주는 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태학적 파싱 : 두가지 구성 요소를 분리하는 작업\n",
    "- cats라는 단어에 대해 형태학적 파싱을 수행\n",
    "- 형태학적 파싱 : cats -- > cat(어간) -s(접사) 분리\n",
    "- eg2) fox는 형태학적 파싱을 하더라도 더 이상 분리할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://wikidocs.net/21707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([lemmatizer.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
